{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads definitions and ignores those with no age assigned. A manually lemmatized version of the definitions is available and I work directly with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age median: 12.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEKCAYAAADq59mMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZnlIklr7/3b4RnwEB7NJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe4fd185e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "cesca_all = pd.read_csv('cesca/definitions.csv')\n",
    "cesca_filtered = cesca_all.dropna(axis=0, subset=['age', 'lemmatized'])  # removes rows with na in either age or lemmatized fields\n",
    "cesca_filtered = cesca_filtered[cesca_filtered['age'] > 0].reset_index(drop=True)  # removes rows with age == 0\n",
    "# some usage comments: dropna and reset_index return new objects, unless 'inplace' is specified\n",
    "cesca_filtered.title = cesca_filtered.title.apply(str.lstrip)  # removes leading whitespaces\n",
    "age_hist = cesca_filtered.age.hist()\n",
    "age_hist.set_xlabel('Edad (a√±os)')\n",
    "age_hist.set_ylabel('f')\n",
    "# cesca_filtered.age.hist(by=cesca_filtered.title)\n",
    "print('Age median: ' + str(np.median(cesca_filtered.age)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gent gran not in lemmas!\n",
      "espantar-se not in lemmas!\n"
     ]
    }
   ],
   "source": [
    "# Are titles lemmatized? Do they appear in the lemmatized version of the definitions?\n",
    "lemmas = []\n",
    "for lemmatized in cesca_filtered.lemmatized:\n",
    "    for lemma in lemmatized.split():\n",
    "        lemmas.append(lemma)\n",
    "lemmas = set(lemmas)\n",
    "for title in cesca_filtered.title.unique():\n",
    "    if title not in lemmas:\n",
    "        print(title + ' not in lemmas!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace titles not in lemmatized definitions with a lemmatized version\n",
    "lemmatized_titles = cesca_filtered.title.copy()\n",
    "lemmatized_titles[lemmatized_titles == 'gent gran'] = 'gent_gran'  # 'gent_gran' is however not a lemma in lemmatized definitions either;\n",
    "lemmatized_titles[lemmatized_titles == 'espantar-se'] = 'espantar'  # 'espantar' is, indeed, a lemma in lemmatized definitions\n",
    "\n",
    "# and add lemmatized titles at the beginning of the lemmatized definitions\n",
    "cesca_filtered['lemmatized'] = lemmatized_titles + ' ' + cesca_filtered['lemmatized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loads list of stopwords\n",
    "with open('ca_stop3_custom') as stop_ca:\n",
    "    for row in stop_ca:\n",
    "        if not row.startswith('#'):\n",
    "            row = row[:-1]  # drops newline character at the end of the line\n",
    "            stoplist = row.split(', ')\n",
    "stoplist = set(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Younger group has 2352 definitions, while older has 3029.\n"
     ]
    }
   ],
   "source": [
    "# splits dataframe in two by age\n",
    "young_range = (0,11)\n",
    "older_range = (12,99)\n",
    "def split_by_age(df, young_range, older_range):\n",
    "    cesca_young = df[(df.age >= young_range[0]) & (df.age <= young_range[1])]\n",
    "    cesca_older = df[(df.age >= older_range[0]) & (df.age <= older_range[1])]\n",
    "    return cesca_young, cesca_older\n",
    "\n",
    "cesca_young, cesca_older = split_by_age(cesca_filtered, young_range, older_range)\n",
    "print('Younger group has {} definitions, while older has {}.'.format(len(cesca_young), len(cesca_older)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Young vocabulary has 512 tokens, including bi- and trigrams if applicable.\n",
      "Older vocabulary has 854 tokens, including bi- and trigrams if applicable.\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.feature_extraction.text\n",
    "TfidfVectorizer = sklearn.feature_extraction.text.TfidfVectorizer\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "def tokenizer(string):\n",
    "    # if left to the CountVectorizer's default tokenizer, it splits words with pont volat\n",
    "    tokens = [token for token in string.split() if len(token) > 1]  # it only keeps tokens longer than one character\n",
    "    #tokens = [token for token in string.replace('_', ' ').split() if len(token) > 1]  # this breaks 2+gram lemmas with underscores,\n",
    "                                                                                       # but it may not be the best choice as they\n",
    "                                                                                       # were manually annotated\n",
    "    return tokens\n",
    "\n",
    "# as opposed to CountVectorizer, TfidfVectorizer normalizes by default\n",
    "vectorizer_young = TfidfVectorizer(\n",
    "                                   binary=False,  # very short texts are likely to have noisy tf‚Äìidf values while the binary occurrence info is more stable\n",
    "                                   lowercase=False,\n",
    "                                   max_df=1.0,  # ignore terms with document frequency higher than this threshold\n",
    "                                   min_df=3,  # ignore terms with document frequency lower than threshold\n",
    "                                   preprocessor=None,  # override the preprocessing step\n",
    "                                   tokenizer=tokenizer,  # override the tokenizer step; wouldn't override the ngram_range parameter\n",
    "                                   ngram_range=(1,1),  # extract n-grams\n",
    "                                   stop_words=stoplist,  # list of stopwords\n",
    "                                   strip_accents=None,\n",
    "                                   token_pattern='',  # The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored);\n",
    "                                                      # should be overriden by custom tokenizer\n",
    "                                   use_idf = True,\n",
    "                                   norm = 'l2', # disable normalization and idf to get same results as with CountVectorizer\n",
    "                                   )\n",
    "vectorizer_older = deepcopy(vectorizer_young)\n",
    "\n",
    "tfidf_young = vectorizer_young.fit_transform(cesca_young.lemmatized)\n",
    "tfidf_older = vectorizer_older.fit_transform(cesca_older.lemmatized)\n",
    "\n",
    "print('Young vocabulary has {} tokens, including bi- and trigrams if applicable.'.format(len(vectorizer_young.get_feature_names())))\n",
    "print('Older vocabulary has {} tokens, including bi- and trigrams if applicable.'.format(len(vectorizer_older.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step:\n",
      "Young group: 44.98721500747006%\n",
      "Older group: 38.14305763545032%\n"
     ]
    }
   ],
   "source": [
    "# LSA dimensionality reduction\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "n_components = 50  # 100 is recommended number of components in SKLearn documentation\n",
    "                    # Gensim recommends 200-500\n",
    "svd_young = TruncatedSVD(n_components)\n",
    "normalizer_young = Normalizer(copy=False)  # rescale each row/doc so that its norm (l2-norm by default) equals to one\n",
    "lsaer_young = make_pipeline(svd_young, normalizer_young)\n",
    "\n",
    "svd_older = deepcopy(svd_young)\n",
    "normalizer_older = deepcopy(normalizer_young)\n",
    "lsaer_older = make_pipeline(svd_older, normalizer_older)\n",
    "\n",
    "lsa_young = lsaer_young.fit_transform(tfidf_young)\n",
    "lsa_older = lsaer_older.fit_transform(tfidf_older)\n",
    "\n",
    "print('Explained variance of the SVD step:')\n",
    "print('Young group: {}%'.format(svd_young.explained_variance_.sum() * 100))\n",
    "print('Older group: {}%'.format(svd_older.explained_variance_.sum() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one set of features from young and older vectorizers; intersection\n",
    "features_all = set(vectorizer_young.get_feature_names()).intersection(vectorizer_older.get_feature_names())\n",
    "# Warning: this set should be used in all shuffling steps, given that the features in each vectorizer may not be\n",
    "# exactly the same (e.g. it may happen that some feature appears only once in one of the random young/older subsets\n",
    "# and is thus ignored)\n",
    "features_all = sorted(list(features_all))  # sort features alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarities(features, vectorizer_young, vectorizer_older, lsaer_young, lsaer_older):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "    from scipy.stats import spearmanr\n",
    "        \n",
    "    features_tfidf_young = vectorizer_young.transform(features)\n",
    "    features_lsa_young = lsaer_young.transform(features_tfidf_young)\n",
    "    empty_young = [feature not in vectorizer_young.get_feature_names() for feature in features]  # logical indices of features not in vocabulary\n",
    "    \n",
    "    features_tfidf_older = vectorizer_older.transform(features)\n",
    "    features_lsa_older = lsaer_older.transform(features_tfidf_older)\n",
    "    empty_older = [feature not in vectorizer_older.get_feature_names() for feature in features]\n",
    "    \n",
    "    similarity_young = cosine_similarity(features_lsa_young)  # On L2-normalized data, this function is equivalent to linear_kernel\n",
    "    # clears distances which involve features not in the vocabulary\n",
    "    # this is needed for shuffled data as feature intersection (features_all) is obtained only once\n",
    "    similarity_young[empty_young, :] = np.nan\n",
    "    similarity_young[:, empty_young] = np.nan\n",
    "    \n",
    "    similarity_older = cosine_similarity(features_lsa_older)\n",
    "    similarity_older[empty_older, :] = np.nan\n",
    "    similarity_older[:, empty_older] = np.nan\n",
    "    \n",
    "    # for each pair of lemmas, it calculates the distance (between lemmas) difference between both groups (young and older)\n",
    "    similarity_delta = similarity_older - similarity_young\n",
    "    \n",
    "    return similarity_young, similarity_older, similarity_delta\n",
    "\n",
    "similarity_young, similarity_older, similarity_delta = get_similarities(features_all,\n",
    "                                                                        vectorizer_young, vectorizer_older,\n",
    "                                                                        lsaer_young, lsaer_older)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Young: 0.031068100422697673 ¬± 0.1603492896207648\n",
      "Older: 0.02758605988164162 ¬± 0.16141702464514926\n"
     ]
    }
   ],
   "source": [
    "# Apparently, it is not obvious that measurements of distance can be\n",
    "# compared between vector/semantic spaces.\n",
    "# Are distributions of distances comparable between corpuses/age-groups?\n",
    "print('Young: {} ¬± {}'.format(np.mean(similarity_young), np.std(similarity_young)))\n",
    "print('Older: {} ¬± {}'.format(np.mean(similarity_older), np.std(similarity_older)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "iter = 100\n",
    "similarity_deltas = []\n",
    "similarity_correlations = []\n",
    "\n",
    "# copy vectorizers and lsaers for iterations, to keep original fits with unshuffled data\n",
    "vectorizer_young_shuffle = deepcopy(vectorizer_young)\n",
    "vectorizer_older_shuffle = deepcopy(vectorizer_older)\n",
    "\n",
    "svd_young_shuffle = deepcopy(svd_young)\n",
    "normalizer_young_shuffle = deepcopy(normalizer_young)\n",
    "lsaer_young_shuffle = make_pipeline(svd_young_shuffle, normalizer_young_shuffle)\n",
    "\n",
    "svd_older_shuffle = deepcopy(svd_older)\n",
    "normalizer_older_shuffle = deepcopy(normalizer_older)\n",
    "lsaer_older_shuffle = make_pipeline(svd_older_shuffle, normalizer_older_shuffle)\n",
    "\n",
    "for i in range(iter):\n",
    "    print(i)\n",
    "    cesca_shuffled = cesca_filtered.copy()\n",
    "    cesca_shuffled.age = cesca_shuffled.age.sample(frac=1).reset_index(drop=True)  # shuffles ages\n",
    "    young_shuffled, older_shuffled = split_by_age(cesca_shuffled, young_range, older_range)\n",
    "    \n",
    "    tfidf_young_shuffle = vectorizer_young_shuffle.fit_transform(young_shuffled.lemmatized)\n",
    "    tfidf_older_shuffle = vectorizer_older_shuffle.fit_transform(older_shuffled.lemmatized)\n",
    "    lsa_young_shuffle = lsaer_young_shuffle.fit_transform(tfidf_young_shuffle)\n",
    "    lsa_older_shuffle = lsaer_older_shuffle.fit_transform(tfidf_older_shuffle)\n",
    "    _, _, shuffled_delta = get_similarities(features_all,\n",
    "                                            vectorizer_young_shuffle, vectorizer_older_shuffle,\n",
    "                                            lsaer_young_shuffle, lsaer_older_shuffle)\n",
    "    similarity_deltas.append(shuffled_delta)\n",
    "\n",
    "similarity_deltas = np.array(similarity_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find significant deltas\n",
    "delta_means = np.nanmean(similarity_deltas, axis=0)  # ignore nan\n",
    "delta_stds = np.nanstd(similarity_deltas, axis=0)  # ignore nan\n",
    "\n",
    "delta_z = (similarity_delta - delta_means) / delta_stds\n",
    "delta_z[np.tril_indices(delta_z.shape[0])] = 0  # dismiss lower-left triangle and diagonal values\n",
    "significant_deltas = np.nonzero(delta_z > 4)  # keeps significant deltas (> 4sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focus on selected target features and list lemmas whose distance has changed significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'astre' and 'constel¬∑laci√≥' converge with development.\n",
      "'constel¬∑laci√≥' and 'espai' converge with development.\n",
      "'constel¬∑laci√≥' and 'planeta' converge with development.\n",
      "'constel¬∑laci√≥' and 'univers' converge with development.\n",
      "'envoltat' and 'escola' converge with development.\n",
      "'envoltat' and 'terra' converge with development.\n",
      "'escola' and 'terra' converge with development.\n"
     ]
    }
   ],
   "source": [
    "# loads target features (selected features I want to focus on)\n",
    "with open('cesca/target_features.txt') as filein:\n",
    "    target_features = filein.readlines()\n",
    "target_features = [feature.strip() for feature in target_features]\n",
    "\n",
    "# iterate over each pair of features whose distance changed significantly between groups,\n",
    "# check that both belong to the list of selected/target words, and\n",
    "# print in what direction the change occurred (i.e. in which group they were closer)\n",
    "for i, j in np.nditer(significant_deltas):\n",
    "    feat_1 = features_all[i]\n",
    "    feat_2 = features_all[j]\n",
    "    if feat_1 in target_features and feat_2 in target_features:\n",
    "        if similarity_young[i, j] < similarity_older[i, j]:\n",
    "            print(\"'{}' and '{}' converge with development.\".format(feat_1, feat_2))\n",
    "        elif similarity_young[i, j] > similarity_older[i, j]:\n",
    "            print(\"'{}' and '{}' diverge with development.\".format(feat_1, feat_2))\n",
    "        else:\n",
    "            print(\"Unexpected result for features {} and {}.\".format(feat_1, feat_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudio de caso: constel¬∑laci√≥\n",
    "Usando los siguientes par√°metros:\n",
    "- young_range = (0, 11); older_range = (12, 99), \n",
    "- TfidfVectorizer(binary=False, use_idf=True, min_df=3), \n",
    "- n_components = 50,  \n",
    "\n",
    "encuentro que la distancia entre constel¬∑laci√≥ y una serie de t√©rminos astron√≥micos (astre, espai, infinit, planeta, univers) difiere significativamente entre grupos. Me pregunto, ¬øse debe esto a una frecuencia de uso distinta de la palabra constel¬∑laci√≥ entre grupos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document frequency of word \"constel¬∑laci√≥\" in \"young\" subcorpus is 5\n",
      "Document frequency of word \"constel¬∑laci√≥\" in \"older\" subcorpus is 11\n"
     ]
    }
   ],
   "source": [
    "# what's the frequency of word constel¬∑laci√≥ in both corpuses?\n",
    "\n",
    "from collections import Counter\n",
    "counter_young = Counter()  # counter holds the total number of times that words appear in the corpus\n",
    "counter_older = Counter()\n",
    "df_young = Counter()  # df (document frequency) hold the number of documents in which the words appear\n",
    "df_older = Counter()\n",
    "for i in range(len(cesca_young)):\n",
    "    this_count = Counter(cesca_young.lemmatized.iloc[i].split())\n",
    "    counter_young += this_count\n",
    "    df_young += {k: 1 for k in this_count}  # binarizes count before adding to cumulative count\n",
    "for i in range(len(cesca_older)):\n",
    "    this_count = Counter(cesca_older.lemmatized.iloc[i].split())\n",
    "    counter_older += this_count\n",
    "    df_older += {k: 1 for k in this_count}\n",
    "\n",
    "target_word = 'constel¬∑laci√≥'\n",
    "print('Document frequency of word \"{}\" in \"young\" subcorpus is {}'.format(target_word, df_young[target_word]))\n",
    "print('Document frequency of word \"{}\" in \"older\" subcorpus is {}'.format(target_word, df_older[target_word]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
