{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads definitions and ignores those with no age assigned. A manually lemmatized version of the definitions is available and I work directly with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age median: 12.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "cesca_all = pd.read_csv('cesca/definitions.csv')\n",
    "cesca_filtered = cesca_all.dropna(axis=0, subset=['age', 'lemmatized'])  # removes rows with na in either age or lemmatized fields\n",
    "cesca_filtered = cesca_filtered[cesca_filtered['age'] > 0].reset_index(drop=True)  # removes rows with age == 0\n",
    "# some usage comments: dropna and reset_index return new objects, unless 'inplace' is specified\n",
    "cesca_filtered.title = cesca_filtered.title.apply(str.lstrip)  # removes leading whitespaces\n",
    "age_hist = cesca_filtered.age.hist()\n",
    "age_hist.set_xlabel('Edad (años)')\n",
    "age_hist.set_ylabel('f')\n",
    "# cesca_filtered.age.hist(by=cesca_filtered.title)\n",
    "print('Age median: ' + str(np.median(cesca_filtered.age)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gent gran not in lemmas!\n",
      "espantar-se not in lemmas!\n"
     ]
    }
   ],
   "source": [
    "# Are titles lemmatized? Do they appear in the lemmatized version of the definitions?\n",
    "lemmas = []\n",
    "for lemmatized in cesca_filtered.lemmatized:\n",
    "    for lemma in lemmatized.split():\n",
    "        lemmas.append(lemma)\n",
    "lemmas = set(lemmas)\n",
    "for title in cesca_filtered.title.unique():\n",
    "    if title not in lemmas:\n",
    "        print(title + ' not in lemmas!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace titles not in lemmatized definitions with a lemmatized version\n",
    "lemmatized_titles = cesca_filtered.title.copy()\n",
    "lemmatized_titles[lemmatized_titles == 'gent gran'] = 'gent_gran'  # 'gent_gran' is however not a lemma in lemmatized definitions either;\n",
    "lemmatized_titles[lemmatized_titles == 'espantar-se'] = 'espantar'  # 'espantar' is, indeed, a lemma in lemmatized definitions\n",
    "\n",
    "# and add lemmatized titles at the beginning of the lemmatized definitions\n",
    "cesca_filtered['lemmatized'] = lemmatized_titles + ' ' + cesca_filtered['lemmatized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads list of stopwords\n",
    "with open('ca_stop3_custom') as stop_ca:\n",
    "    for row in stop_ca:\n",
    "        if not row.startswith('#'):\n",
    "            row = row[:-1]  # drops newline character at the end of the line\n",
    "            stoplist = row.split(', ')\n",
    "stoplist = set(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Younger group has 2352 definitions, while older has 3029.\n"
     ]
    }
   ],
   "source": [
    "# splits dataframe in two by age\n",
    "young_range = (0,11)\n",
    "older_range = (12,99)\n",
    "def split_by_age(df, young_range, older_range):\n",
    "    cesca_young = df[(df.age >= young_range[0]) & (df.age <= young_range[1])]\n",
    "    cesca_older = df[(df.age >= older_range[0]) & (df.age <= older_range[1])]\n",
    "    return cesca_young, cesca_older\n",
    "\n",
    "cesca_young, cesca_older = split_by_age(cesca_filtered, young_range, older_range)\n",
    "print('Younger group has {} definitions, while older has {}.'.format(len(cesca_young), len(cesca_older)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Young vocabulary has 512 tokens, including bi- and trigrams if applicable.\n",
      "Older vocabulary has 854 tokens, including bi- and trigrams if applicable.\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.feature_extraction.text\n",
    "TfidfVectorizer = sklearn.feature_extraction.text.TfidfVectorizer\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "def tokenizer(string):\n",
    "    # if left to the CountVectorizer's default tokenizer, it splits words with pont volat\n",
    "    tokens = [token for token in string.split() if len(token) > 1]  # it only keeps tokens longer than one character\n",
    "    #tokens = [token for token in string.replace('_', ' ').split() if len(token) > 1]  # this breaks 2+gram lemmas with underscores,\n",
    "                                                                                       # but it may not be the best choice as they\n",
    "                                                                                       # were manually annotated\n",
    "    return tokens\n",
    "\n",
    "# as opposed to CountVectorizer, TfidfVectorizer normalizes by default\n",
    "vectorizer_young = TfidfVectorizer(\n",
    "                                   binary=False,  # very short texts are likely to have noisy tf–idf values while the binary occurrence info is more stable\n",
    "                                   lowercase=False,\n",
    "                                   max_df=1.0,  # ignore terms with document frequency higher than this threshold\n",
    "                                   min_df=3,  # ignore terms with document frequency lower than threshold\n",
    "                                   preprocessor=None,  # override the preprocessing step\n",
    "                                   tokenizer=tokenizer,  # override the tokenizer step; wouldn't override the ngram_range parameter\n",
    "                                   ngram_range=(1,1),  # extract n-grams\n",
    "                                   stop_words=stoplist,  # list of stopwords\n",
    "                                   strip_accents=None,\n",
    "                                   token_pattern='',  # The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored);\n",
    "                                                      # should be overriden by custom tokenizer\n",
    "                                   use_idf = True,\n",
    "                                   norm = 'l2', # disable normalization and idf to get same results as with CountVectorizer\n",
    "                                   )\n",
    "vectorizer_older = deepcopy(vectorizer_young)\n",
    "\n",
    "tfidf_young = vectorizer_young.fit_transform(cesca_young.lemmatized)\n",
    "tfidf_older = vectorizer_older.fit_transform(cesca_older.lemmatized)\n",
    "\n",
    "print('Young vocabulary has {} tokens, including bi- and trigrams if applicable.'.format(len(vectorizer_young.get_feature_names())))\n",
    "print('Older vocabulary has {} tokens, including bi- and trigrams if applicable.'.format(len(vectorizer_older.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step:\n",
      "Young group: 45.01736574606577%\n",
      "Older group: 38.15392276933423%\n"
     ]
    }
   ],
   "source": [
    "# LSA dimensionality reduction\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "n_components = 50  # 100 is recommended number of components in SKLearn documentation\n",
    "                    # Gensim recommends 200-500\n",
    "svd_young = TruncatedSVD(n_components)\n",
    "normalizer_young = Normalizer(copy=False)  # rescale each row/doc so that its norm (l2-norm by default) equals to one\n",
    "lsaer_young = make_pipeline(svd_young, normalizer_young)\n",
    "\n",
    "svd_older = deepcopy(svd_young)\n",
    "normalizer_older = deepcopy(normalizer_young)\n",
    "lsaer_older = make_pipeline(svd_older, normalizer_older)\n",
    "\n",
    "lsa_young = lsaer_young.fit_transform(tfidf_young)\n",
    "lsa_older = lsaer_older.fit_transform(tfidf_older)\n",
    "\n",
    "print('Explained variance of the SVD step:')\n",
    "print('Young group: {}%'.format(svd_young.explained_variance_.sum() * 100))\n",
    "print('Older group: {}%'.format(svd_older.explained_variance_.sum() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one set of features from young and older vectorizers; intersection\n",
    "features_all = set(vectorizer_young.get_feature_names()).intersection(vectorizer_older.get_feature_names())\n",
    "# Warning: this set should be used in all shuffling steps, given that the features in each vectorizer may not be\n",
    "# exactly the same (e.g. it may happen that some feature appears only once in one of the random young/older subsets\n",
    "# and is thus ignored)\n",
    "features_all = sorted(list(features_all))  # sort features alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarities(features, vectorizer_young, vectorizer_older, lsaer_young, lsaer_older):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "    from scipy.stats import spearmanr\n",
    "        \n",
    "    features_tfidf_young = vectorizer_young.transform(features)\n",
    "    features_lsa_young = lsaer_young.transform(features_tfidf_young)\n",
    "    empty_young = [feature not in vectorizer_young.get_feature_names() for feature in features]  # logical indices of features not in vocabulary\n",
    "    \n",
    "    features_tfidf_older = vectorizer_older.transform(features)\n",
    "    features_lsa_older = lsaer_older.transform(features_tfidf_older)\n",
    "    empty_older = [feature not in vectorizer_older.get_feature_names() for feature in features]\n",
    "    \n",
    "    similarity_young = cosine_similarity(features_lsa_young)  # On L2-normalized data, this function is equivalent to linear_kernel\n",
    "    # clears distances which involve features not in the vocabulary\n",
    "    # this is needed for shuffled data as feature intersection (features_all) is obtained only once\n",
    "    similarity_young[empty_young, :] = np.nan\n",
    "    similarity_young[:, empty_young] = np.nan\n",
    "    \n",
    "    similarity_older = cosine_similarity(features_lsa_older)\n",
    "    similarity_older[empty_older, :] = np.nan\n",
    "    similarity_older[:, empty_older] = np.nan\n",
    "    \n",
    "    # for each pair of lemmas, it calculates the distance (between lemmas) difference between both groups (young and older)\n",
    "    similarity_delta = similarity_older - similarity_young\n",
    "    \n",
    "    return similarity_young, similarity_older, similarity_delta\n",
    "\n",
    "similarity_young, similarity_older, similarity_delta = get_similarities(features_all,\n",
    "                                                                        vectorizer_young, vectorizer_older,\n",
    "                                                                        lsaer_young, lsaer_older)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Young: 0.029524682038387266 ± 0.15940152854377468\n",
      "Older: 0.02938738226089422 ± 0.1608434571461749\n"
     ]
    }
   ],
   "source": [
    "# Apparently, it is not obvious that measurements of distance can be\n",
    "# compared between vector/semantic spaces.\n",
    "# Are distributions of distances comparable between corpuses/age-groups?\n",
    "print('Young: {} ± {}'.format(np.mean(similarity_young), np.std(similarity_young)))\n",
    "print('Older: {} ± {}'.format(np.mean(similarity_older), np.std(similarity_older)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "iter = 1000\n",
    "similarity_deltas = []\n",
    "similarity_correlations = []\n",
    "\n",
    "# copy vectorizers and lsaers for iterations, to keep original fits with unshuffled data\n",
    "vectorizer_young_shuffle = deepcopy(vectorizer_young)\n",
    "vectorizer_older_shuffle = deepcopy(vectorizer_older)\n",
    "\n",
    "svd_young_shuffle = deepcopy(svd_young)\n",
    "normalizer_young_shuffle = deepcopy(normalizer_young)\n",
    "lsaer_young_shuffle = make_pipeline(svd_young_shuffle, normalizer_young_shuffle)\n",
    "\n",
    "svd_older_shuffle = deepcopy(svd_older)\n",
    "normalizer_older_shuffle = deepcopy(normalizer_older)\n",
    "lsaer_older_shuffle = make_pipeline(svd_older_shuffle, normalizer_older_shuffle)\n",
    "\n",
    "for i in range(iter):\n",
    "    print(i)\n",
    "    cesca_shuffled = cesca_filtered.copy()\n",
    "    cesca_shuffled.age = cesca_shuffled.age.sample(frac=1).reset_index(drop=True)  # shuffles ages\n",
    "    young_shuffled, older_shuffled = split_by_age(cesca_shuffled, young_range, older_range)\n",
    "    \n",
    "    tfidf_young_shuffle = vectorizer_young_shuffle.fit_transform(young_shuffled.lemmatized)\n",
    "    tfidf_older_shuffle = vectorizer_older_shuffle.fit_transform(older_shuffled.lemmatized)\n",
    "    lsa_young_shuffle = lsaer_young_shuffle.fit_transform(tfidf_young_shuffle)\n",
    "    lsa_older_shuffle = lsaer_older_shuffle.fit_transform(tfidf_older_shuffle)\n",
    "    _, _, shuffled_delta = get_similarities(features_all,\n",
    "                                            vectorizer_young_shuffle, vectorizer_older_shuffle,\n",
    "                                            lsaer_young_shuffle, lsaer_older_shuffle)\n",
    "    similarity_deltas.append(shuffled_delta)\n",
    "\n",
    "similarity_deltas = np.array(similarity_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find significant deltas\n",
    "delta_means = np.nanmean(similarity_deltas, axis=0)  # ignore nan\n",
    "delta_stds = np.nanstd(similarity_deltas, axis=0)  # ignore nan\n",
    "\n",
    "delta_z = (similarity_delta - delta_means) / delta_stds\n",
    "delta_z[np.tril_indices(delta_z.shape[0])] = 0  # dismiss lower-left triangle and diagonal values\n",
    "significant_deltas = np.nonzero(delta_z > 4)  # keeps significant deltas (> 4sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focus on selected target features and list lemmas whose distance has changed significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'astre' and 'constel·lació' converge with development.\n",
      "'constel·lació' and 'espai' converge with development.\n",
      "'constel·lació' and 'infinit' converge with development.\n",
      "'constel·lació' and 'planeta' converge with development.\n",
      "'constel·lació' and 'univers' converge with development.\n",
      "'envoltat' and 'terra' converge with development.\n",
      "'illa' and 'lluny' converge with development.\n",
      "'illa' and 'llunyà' converge with development.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>dist_young</th>\n",
       "      <th>dist_old</th>\n",
       "      <th>dist_delta</th>\n",
       "      <th>delta_mean</th>\n",
       "      <th>delta_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>astre</td>\n",
       "      <td>constel·lació</td>\n",
       "      <td>0.429151</td>\n",
       "      <td>0.008137</td>\n",
       "      <td>-0.421014</td>\n",
       "      <td>-0.018512</td>\n",
       "      <td>0.096460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>constel·lació</td>\n",
       "      <td>espai</td>\n",
       "      <td>0.496887</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>-0.480887</td>\n",
       "      <td>-0.022817</td>\n",
       "      <td>0.083876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>constel·lació</td>\n",
       "      <td>infinit</td>\n",
       "      <td>0.607468</td>\n",
       "      <td>0.010766</td>\n",
       "      <td>-0.596702</td>\n",
       "      <td>-0.018566</td>\n",
       "      <td>0.134418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>constel·lació</td>\n",
       "      <td>planeta</td>\n",
       "      <td>0.464812</td>\n",
       "      <td>0.006977</td>\n",
       "      <td>-0.457835</td>\n",
       "      <td>-0.015468</td>\n",
       "      <td>0.079483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>constel·lació</td>\n",
       "      <td>univers</td>\n",
       "      <td>0.619550</td>\n",
       "      <td>0.011329</td>\n",
       "      <td>-0.608221</td>\n",
       "      <td>-0.018749</td>\n",
       "      <td>0.130208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>envoltat</td>\n",
       "      <td>terra</td>\n",
       "      <td>0.264417</td>\n",
       "      <td>0.026088</td>\n",
       "      <td>-0.238329</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>0.039436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>illa</td>\n",
       "      <td>lluny</td>\n",
       "      <td>1.282361</td>\n",
       "      <td>0.922646</td>\n",
       "      <td>-0.359715</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.063544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>illa</td>\n",
       "      <td>llunyà</td>\n",
       "      <td>1.288159</td>\n",
       "      <td>0.924155</td>\n",
       "      <td>-0.364004</td>\n",
       "      <td>-0.000549</td>\n",
       "      <td>0.063897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          feat_1         feat_2  dist_young  dist_old  dist_delta  delta_mean  \\\n",
       "0          astre  constel·lació    0.429151  0.008137   -0.421014   -0.018512   \n",
       "1  constel·lació          espai    0.496887  0.016000   -0.480887   -0.022817   \n",
       "2  constel·lació        infinit    0.607468  0.010766   -0.596702   -0.018566   \n",
       "3  constel·lació        planeta    0.464812  0.006977   -0.457835   -0.015468   \n",
       "4  constel·lació        univers    0.619550  0.011329   -0.608221   -0.018749   \n",
       "5       envoltat          terra    0.264417  0.026088   -0.238329   -0.006806   \n",
       "6           illa          lluny    1.282361  0.922646   -0.359715    0.000332   \n",
       "7           illa         llunyà    1.288159  0.924155   -0.364004   -0.000549   \n",
       "\n",
       "   delta_std  \n",
       "0   0.096460  \n",
       "1   0.083876  \n",
       "2   0.134418  \n",
       "3   0.079483  \n",
       "4   0.130208  \n",
       "5   0.039436  \n",
       "6   0.063544  \n",
       "7   0.063897  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loads target features (selected features I want to focus on)\n",
    "with open('cesca/target_features.txt') as filein:\n",
    "    target_features = filein.readlines()\n",
    "target_features = [feature.strip() for feature in target_features]\n",
    "\n",
    "df_out = pd.DataFrame(\n",
    "    columns=[\n",
    "        'feat_1',\n",
    "        'feat_2',\n",
    "        'dist_young',\n",
    "        'dist_old',\n",
    "        'dist_delta',\n",
    "        'delta_mean',\n",
    "        'delta_std'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# iterate over each pair of features whose distance changed significantly between groups,\n",
    "# check that both belong to the list of selected/target words, and\n",
    "# print in what direction the change occurred (i.e. in which group they were closer)\n",
    "for i, j in np.nditer(significant_deltas):\n",
    "    feat_1 = features_all[i]\n",
    "    feat_2 = features_all[j]\n",
    "    if feat_1 in target_features and feat_2 in target_features:\n",
    "        if similarity_young[i, j] < similarity_older[i, j]:\n",
    "            print(\"'{}' and '{}' converge with development.\".format(feat_1, feat_2))\n",
    "        elif similarity_young[i, j] > similarity_older[i, j]:\n",
    "            print(\"'{}' and '{}' diverge with development.\".format(feat_1, feat_2))\n",
    "        else:\n",
    "            print(\"Unexpected result for features {} and {}.\".format(feat_1, feat_2))\n",
    "        df_out = df_out.append({\n",
    "            'feat_1': feat_1,\n",
    "            'feat_2': feat_2,\n",
    "            'dist_young': 1 - similarity_young[i, j],\n",
    "            'dist_old': 1 - similarity_older[i, j],\n",
    "            'dist_delta': - similarity_delta[i, j],\n",
    "            'delta_mean': - delta_means[i, j],\n",
    "            'delta_std': delta_stds[i, j]\n",
    "        }, ignore_index=True)\n",
    "\n",
    "df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudio de caso: constel·lació\n",
    "Usando los siguientes parámetros:\n",
    "- young_range = (0, 11); older_range = (12, 99), \n",
    "- TfidfVectorizer(binary=False, use_idf=True, min_df=3), \n",
    "- n_components = 50,  \n",
    "\n",
    "encuentro que la distancia entre constel·lació y una serie de términos astronómicos (astre, espai, infinit, planeta, univers) difiere significativamente entre grupos. Me pregunto, ¿se debe esto a una frecuencia de uso distinta de la palabra constel·lació entre grupos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document frequency of word \"constel·lació\" in \"young\" subcorpus is 5\n",
      "Document frequency of word \"constel·lació\" in \"older\" subcorpus is 11\n"
     ]
    }
   ],
   "source": [
    "# what's the frequency of word constel·lació in both corpuses?\n",
    "\n",
    "from collections import Counter\n",
    "counter_young = Counter()  # counter holds the total number of times that words appear in the corpus\n",
    "counter_older = Counter()\n",
    "df_young = Counter()  # df (document frequency) hold the number of documents in which the words appear\n",
    "df_older = Counter()\n",
    "for i in range(len(cesca_young)):\n",
    "    this_count = Counter(cesca_young.lemmatized.iloc[i].split())\n",
    "    counter_young += this_count\n",
    "    df_young += {k: 1 for k in this_count}  # binarizes count before adding to cumulative count\n",
    "for i in range(len(cesca_older)):\n",
    "    this_count = Counter(cesca_older.lemmatized.iloc[i].split())\n",
    "    counter_older += this_count\n",
    "    df_older += {k: 1 for k in this_count}\n",
    "\n",
    "target_word = 'constel·lació'\n",
    "print('Document frequency of word \"{}\" in \"young\" subcorpus is {}'.format(target_word, df_young[target_word]))\n",
    "print('Document frequency of word \"{}\" in \"older\" subcorpus is {}'.format(target_word, df_older[target_word]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
